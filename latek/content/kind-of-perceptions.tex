%----------------------------------------------------------------------------
\chapter{Computer vision}
\label{chap:perceptions}
%----------------------------------------------------------------------------

After collecting data from the sensors we choose we need to implement the right
algorithms to extract information from the sensor data. In this chapter I start
with explaining the basics of computer vision and then move on to advanced
convolutional neural netowrks that will help our goal.

Computer Vision, often abbreviated as CV, is defined as a field of study that
seeks to develop techniques to help computers “see” and understand the content
of digital images such as photographs and videos.

The problem of computer vision appears simple because it is trivially solved by
people, even babies. Nevertheless, it largely remains an unsolved
problem based both on the limited understanding of biological vision and because
of the complexity of vision perception in a dynamic and nearly infinitely
varying physical world.

\section{Challenges in Computer Vision}
Image classification is considered to be the most basic application of computer
vision. The rest of the developments in computer vision are achieved by making
small enhancements on top of this. Since this task is intuitive for us, we fail to
appreciate the key challenges involved when we try to design systems similar to
our eye. Some challenges for computers are:

\begin{itemize}
    \item Variations in viewpoint
    \item Difference in illumination
    \item Hidden parts of images, occulsion
    \item Background Clutter
\end{itemize}

\section{Traditional approaches}

Various techniques, other than deep learning are available in computer vision.
They work well for simpler problems, but as the data becomes huge and the task
becomes more complex, they are no substitute for deep CNNs. Let’s briefly
discuss two simple approaches.

\subsection{KNN (K-Nearest Neighbours)}

In the KNN algorithm each image is matched with all images in training data. The
top K with minimum distances are selected. The majority class of those top K is
predicted as output class of the image. Various distance metrics can be used
like L1 distance (sum of absolute distance), L2 distance (sum of squares), etc.
However KNN performs poorly - qute expectedly - they have a high error rate on
complex images, because all they do is compare pixel values among other images,
without any use of image patterns.

\subsection{Linear Classifiers}

They use a parametric approach where each pixel value is considered as a
parameter. It’s like a weighted sum of the pixel values with the dimension of
the weights matrix depending on the number of outcomes. Intuitively, we can
understand this in terms of a template. The weighted sum of pixels forms a
template image which is matched with every image. This will also face difficulty
in overcoming the challenges discussed in earlier as it is difficult to design a
single template for all the different cases.

\section{Convolutional Neural Networks}

Visual recognition tasks such as image classification, localization, and
detection are key components of computer vision. However these are not possible
to achieve with traditional vision.

Recent developments in neural networks and deep learning approaches have greatly
advanced the performance of these state-of-the-art visual recognition systems.

Neural networks are the basis of deep learning methods. They are made up of
multiple layers, each layer containing multiple perceptrons. Layers can be
fully-connected or sparsely if possible, providing some performance benefits.
Each perceptron is an activation function whose input is the weighted output of
perceptrons from previous layers, and the function is usually a sigmoid
function. A neural network's first layer is the input layer and the last layer is
the output, which could be an array of perceptron where only one yields a high
output creating a classifier. Layer in-between are called hidden layers and it
is up to design and experimentation the determine what is the right
configuration of hidden layers.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=80mm, keepaspectratio]{figures/nn.png}
    \caption{Neural network visualization. Image taken from CS231N Notes}
    \label{fig:convnet}
\end{figure}

Neural Networks (NN) are good at classifying different patterns recieved in the
input layers however they are not sufficient for even image classification,
because in one part the number of inputs is way to high. Consider a high
resolution image with $1000\times 1000\times 3$ pixels, then the NN has 3million input
parameters to process. This takes a long time and too much computational power.

Secondly the neural network architecture in itself is not a
general-enough solution (if you think about it, it is similar to a linear
classifier or a KNN).

Convolutional Neural Network (CNNs) however solve image classification and more.
A CNN is able to capture the spatial features in an image through the
application of relevant filters. The architecture performs a better fitting to
an image dataset due to the reduction in the number of parameters involved and
reusability of weights.


There is material on the internet in abundance about how convolutional neural
networks work, and I have read many of them, but the one I recommend most is the
Stanford course CS231N\footnote{ Stanford CV course CS231N
    \url{https://cs231n.github.io/}}. 

The general architecture of CNN is similar to a cone, where the first layer is
the widest and each layer first convolves multiple filters (which in the
beginning of the CNN correspond to edges and corners) applying ReLU (rectifier,
non-linearity function) then it downsizes the input which is called the max
pooling. This repeated over and over in the end results in a small tensor
which can \emph{then} be fed to the fully-connected (FC) layers (i.e. a neural network)
which acts as the classifier.

Why is this the winner architecture? Because if you think about it the neural
network in the end only has to vote for the presence of the right features in roughly
the right image position, not for each pixel. A visualization of a
CNN's architecture can be seen in \autoref{fig:convnet}.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=150mm, keepaspectratio]{figures/convnet.png}
    \caption{Architecture of a CNN}
    \label{fig:convnet}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=60mm, keepaspectratio]{figures/filters.png}
    \caption{A visualization of the features learned in the first convnet layer in AlexNet~\cite{NIPS2012_4824}. AlexNet was a CNN which revolutionized the field of Deep Learning, and is built from conv layers, max-pooling layers and FC layers. Image taken from CS231N notes.}
    \label{fig:filters}
\end{figure}

There are various architectures that have emerged each incrementally improving
on the previous ones: LeNet~\cite{Lecun98gradient-basedlearning} - the work of
Yann LeCun himself, AlexNet~\cite{NIPS2012_4824}
VGGNet~\cite{DBLP:journals/corr/SimonyanZ14a}
GoogLeNet~\cite{DBLP:journals/corr/SzegedyLJSRAEVR14}
ResNet~\cite{DBLP:journals/corr/HeZRS15}

\subsection{Deep Learning}
Deep learning referes to the procedure of training neural networks and
convolutional neural networks to perform the task at hand accurately. During
deep learning first a dataset is created with training images coupled with
"ground truth" data that is the required prediction for each image. The neural
networks are then fed with the images in batches for a certain number of
iterations - epochs. The weights of the neural network and the filters are
adjusted with the loss function that comes from calculating the error of the
current prediction and the ground truth for each image. This error is then
"backpropagated" which is just another way of saying it is multiplied with the
derivative of each weight in the network and subtracted from it. For filters this means
"filtering filters", so only those filters will stay in the convnet which
resulted in a non-zero gradient in the neural network.

\section{Detection and Segmentation}

\subsection{Object Detection, Localization}

The task to define objects within images usually involves outputting bounding
boxes and labels for individual objects. This differs from the
classification / localization task by applying classification and
localization to many objects instead of just a single dominant object.

If we use the Sliding Window technique like the way we classify and localize
images, we need to apply a CNN to many different crops of the image.

In order to cope with this, researchers have proposed to use regions
instead, which are suggestions of regions that are likely to contain
objects. The first such convnet is called
\textbf{R-CNN}~\cite{DBLP:journals/corr/GirshickDDM13} (Region-based
Convolutional Neural Network).

\begin{figure}[!ht]
    \centering
    \includegraphics[width=80mm, keepaspectratio]{figures/rcnn.jpeg}
    \caption{R-CNN architecture}
    \label{fig:rcnn}
\end{figure}

An immediate descendant to R-CNN is \textbf{Fast
    R-CNN}~\cite{DBLP:journals/corr/Girshick15}, which improves the detection
speed through 2 augmentations: 1) Performing feature extraction before
proposing regions, thus only running one CNN over the entire image, and 2)
Replacing SVM with a softmax layer, thus extending the neural network for
predictions instead of creating a new model.

There are other methods for object detection and localization but in general
they are all based on first feature extraction then classification with
different intermediate procedures.

\begin{itemize}
    \item You Only Look Once (YOLOv4~\cite{Bochkovskiy2020YOLOv4OS} being the latest)
    \item Single Shot MultiBox Detector (SSD)~\cite{DBLP:journals/corr/LiuAESR15}
\end{itemize}

\subsection{Segmentation}
Central to Computer Vision is the process of segmentation, which divides whole
images into pixel groupings which can then be labelled and classified.
Particularly, Semantic Segmentation tries to semantically understand the role of
each pixel in the image (e.g. is it a car, a motorbike, or some other type of
class?). Therefore, unlike classification, we need dense pixel-wise predictions
from the models.

One of the earlier approaches was patch classification through a sliding window,
where each pixel was separately classified into classes using a patch of images
around it. This, however, is very inefficient computationally because we don’t
reuse the shared features between overlapping patches. The solution, instead, is
Fully Convolutional Networks (FCN)~\cite{DBLP:journals/corr/LongSD14}.

\subsection{Instance Segmentation}
Beyond Semantic Segmentation, Instance Segmentation segments different instances
of classes, such as labelling 4 cars with 4 different colors. Instance
segmentation problem is explored at Facebook AI using an architecture known as
Mask R-CNN~\cite{DBLP:journals/corr/HeGDG17}. 

The idea is that since Faster R-CNN works so well for object detection is it
possible to extend it to that is also performs pixel-level segmentation. 

Mask R-CNN adss a branch to Faster R-CNN that outputs a binary mask that says
whether or not a given pixel is part of an object. The branch is a Fully
Convolutional Network on top of a CNN-based feature map.
Detectron2~\cite{wu2019detectron2} a detection framework developed by Facebook,
is based on Mask R-CNN and it is the framework I ended up using.

\section{Tracking}
Object Tracking refers to the process of following a specific object of
interest, or multiple objects, in a given scene. It traditionally has
applications in video and real-world interactions where observations are made
following an initial object detection. Now, it’s crucial to autonomous driving
systems.

Simple Online and Realtime Tracking - SORT~\cite{Wojke2017simple} and Deep
SORT~\cite{Wojke2018deep} Are both based on Kalman filters to use the available
detections and previous predictions to arrive at a best guess of the current
state. Deep SORT extends SORT with the use feature extraction with encoders.
These features are then kept in a dictionary for each object. For each detection
throughout the tracking process a distance is calculated between signatures in
the dictionary and the current object's feature model this way tracking
previously identified objects. 








